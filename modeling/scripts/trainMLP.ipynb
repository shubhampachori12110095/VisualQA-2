{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trainMLP.py codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from random import shuffle\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from spacy.en import English\n",
    "\n",
    "from features import get_questions_matrix_sum, get_images_matrix, get_answers_matrix\n",
    "from utils import grouper, selectFrequentAnswers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_train = open('../data/preprocessed/questions_train2014.txt', 'r').read().decode('utf8').splitlines()\n",
    "answers_train = open('../data/preprocessed/answers_train2014_modal.txt', 'r').read().decode('utf8').splitlines()\n",
    "images_train = open('../data/preprocessed/images_train2014.txt', 'r').read().decode('utf8').splitlines()\n",
    "vgg_model_path = '../features/coco/vgg_feats.mat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248349\n",
      "248349\n",
      "248349\n",
      "Is there a shadow?\n",
      "yes\n",
      "487025\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "\n",
    "print len(questions_train)\n",
    "print len(answers_train)\n",
    "print len(images_train)\n",
    "\n",
    "print questions_train[idx]\n",
    "print answers_train[idx]\n",
    "print images_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation='tanh', batch_size=128, dropout=0.5, f='/Users/jyang/Library/Jupyter/runtime/kernel-a9934ce4-7cd6-4817-8ae6-db9c3ec37332.json', language_only=False, model_save_interval=10, num_epochs=1, num_hidden_layers=3, num_hidden_units=1024)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-num_hidden_units', type=int, default=1024)\n",
    "parser.add_argument('-num_hidden_layers', type=int, default=3)\n",
    "parser.add_argument('-dropout', type=float, default=0.5)\n",
    "parser.add_argument('-activation', type=str, default='tanh')\n",
    "parser.add_argument('-language_only', type=bool, default= False)\n",
    "parser.add_argument('-num_epochs', type=int, default=1)\n",
    "parser.add_argument('-model_save_interval', type=int, default=10)\n",
    "parser.add_argument('-batch_size', type=int, default=128)\n",
    "parser.add_argument('-f', type=str, default='')\n",
    "args = parser.parse_args()\n",
    "print args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxAnswers = 1000\n",
    "questions_train, answers_train, images_train = \\\n",
    "    selectFrequentAnswers(questions_train,answers_train,images_train, maxAnswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_classes: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/labelencoder.pkl', '../models/labelencoder.pkl_01.npy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode the remaining answers\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(answers_train)\n",
    "nb_classes = len(list(labelencoder.classes_))\n",
    "print 'nb_classes:', nb_classes\n",
    "joblib.dump(labelencoder,'../models/labelencoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vgg features\n"
     ]
    }
   ],
   "source": [
    "features_struct = scipy.io.loadmat(vgg_model_path)\n",
    "VGGfeatures = features_struct['feats']\n",
    "print 'loaded vgg features'\n",
    "image_ids = open('../features/coco_vgg_IDMap.txt').read().splitlines()\n",
    "id_map = {}\n",
    "for ids in image_ids:\n",
    "    id_split = ids.split()\n",
    "    id_map[id_split[0]] = int(id_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('378466', 58761),\n",
       " ('378467', 27080),\n",
       " ('287140', 82608),\n",
       " ('376426', 111769),\n",
       " ('378461', 92750),\n",
       " ('13356', 73748),\n",
       " ('258823', 116456),\n",
       " ('370250', 98962),\n",
       " ('370252', 48071),\n",
       " ('89378', 29771)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_map)\n",
    "id_map.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded word2vec features...\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "print 'loaded word2vec features...'\n",
    "img_dim = 4096\n",
    "word_vec_dim = 300\n",
    "\n",
    "model = Sequential()\n",
    "if args.language_only:\n",
    "    model.add(Dense(args.num_hidden_units, input_dim=word_vec_dim, init='uniform'))\n",
    "else:\n",
    "    model.add(Dense(args.num_hidden_units, input_dim=img_dim+word_vec_dim, init='uniform'))\n",
    "model.add(Activation(args.activation))\n",
    "if args.dropout>0:\n",
    "    model.add(Dropout(args.dropout))\n",
    "for i in xrange(args.num_hidden_layers-1):\n",
    "    model.add(Dense(args.num_hidden_units, init='uniform'))\n",
    "    model.add(Activation(args.activation))\n",
    "    if args.dropout>0:\n",
    "        model.add(Dropout(args.dropout))\n",
    "model.add(Dense(nb_classes, init='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "json_string = model.to_json()\n",
    "if args.language_only:\n",
    "    model_file_name = '../models/mlp_language_only_num_hidden_units_' + \\\n",
    "    str(args.num_hidden_units) + '_num_hidden_layers_' + str(args.num_hidden_layers)\n",
    "else:\n",
    "    model_file_name = '../models/mlp_num_hidden_units_' + str(args.num_hidden_units) + \\\n",
    "    '_num_hidden_layers_' + str(args.num_hidden_layers)\n",
    "open(model_file_name  + '.json', 'w').write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'{\"class_name\": \"Sequential\", \"keras_version\": \"1.1.0\", \"config\": [{\"class_name\": \"Dense\", \"config\": {\"W_constraint\": , \"b_constraint\": , \"name\": \"dense_1\", \"output_dim\": 1024, \"activity_regularizer\": , \"trainable\": true, \"init\": \"uniform\", \"bias\": true, \"input_dtype\": \"float32\", \"input_dim\": 4396, \"b_regularizer\": , \"W_regularizer\": , \"activation\": \"linear\", \"batch_input_shape\": [, 4396]}}, {\"class_name\": \"Activation\", \"config\": {\"activation\": \"tanh\", \"trainable\": true, \"name\": \"activation_1\"}}, {\"class_name\": \"Dropout\", \"config\": {\"p\": 0.5, \"trainable\": true, \"name\": \"dropout_1\"}}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": , \"b_constraint\": , \"name\": \"dense_2\", \"activity_regularizer\": , \"trainable\": true, \"init\": \"uniform\", \"bias\": true, \"input_dim\": , \"b_regularizer\": , \"W_regularizer\": , \"activation\": \"linear\", \"output_dim\": 1024}}, {\"class_name\": \"Activation\", \"config\": {\"activation\": \"tanh\", \"trainable\": true, \"name\": \"activation_2\"}}, {\"class_name\": \"Dropout\", \"config\": {\"p\": 0.5, \"trainable\": true, \"name\": \"dropout_2\"}}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": , \"b_constraint\": , \"name\": \"dense_3\", \"activity_regularizer\": , \"trainable\": true, \"init\": \"uniform\", \"bias\": true, \"input_dim\": , \"b_regularizer\": , \"W_regularizer\": , \"activation\": \"linear\", \"output_dim\": 1024}}, {\"class_name\": \"Activation\", \"config\": {\"activation\": \"tanh\", \"trainable\": true, \"name\": \"activation_3\"}}, {\"class_name\": \"Dropout\", \"config\": {\"p\": 0.5, \"trainable\": true, \"name\": \"dropout_3\"}}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": , \"b_constraint\": , \"name\": \"dense_4\", \"activity_regularizer\": , \"trainable\": true, \"init\": \"uniform\", \"bias\": true, \"input_dim\": , \"b_regularizer\": , \"W_regularizer\": , \"activation\": \"linear\", \"output_dim\": 1000}}, {\"class_name\": \"Activation\", \"config\": {\"activation\": \"softmax\", \"trainable\": true, \"name\": \"activation_4\"}}]}'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(json_string.replace('null', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Compilation done...\n"
     ]
    }
   ],
   "source": [
    "print 'Compiling model...'\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print 'Compilation done...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x11a54fed0>\n"
     ]
    }
   ],
   "source": [
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "215424/215375 [==============================] - 389s - train loss: 4.3101   \n"
     ]
    }
   ],
   "source": [
    "print 'Training started...'\n",
    "for k in xrange(args.num_epochs):\n",
    "    #shuffle the data points before going through them\n",
    "    index_shuf = range(len(questions_train))\n",
    "    shuffle(index_shuf)\n",
    "    questions_train = [questions_train[i] for i in index_shuf]\n",
    "    answers_train = [answers_train[i] for i in index_shuf]\n",
    "    images_train = [images_train[i] for i in index_shuf]\n",
    "    progbar = generic_utils.Progbar(len(questions_train))\n",
    "    for qu_batch,an_batch,im_batch in zip(grouper(questions_train, args.batch_size, fillvalue=questions_train[-1]), \n",
    "                                          grouper(answers_train, args.batch_size, fillvalue=answers_train[-1]), \n",
    "                                          grouper(images_train, args.batch_size, fillvalue=images_train[-1])):\n",
    "        X_q_batch = get_questions_matrix_sum(qu_batch, nlp)\n",
    "        if args.language_only:\n",
    "            X_batch = X_q_batch\n",
    "        else:\n",
    "            X_i_batch = get_images_matrix(im_batch, id_map, VGGfeatures)\n",
    "            X_batch = np.hstack((X_q_batch, X_i_batch))\n",
    "        Y_batch = get_answers_matrix(an_batch, labelencoder)\n",
    "        loss = model.train_on_batch(X_batch, Y_batch)\n",
    "        progbar.add(args.batch_size, values=[(\"train loss\", loss)])\n",
    "    #print type(loss)\n",
    "    if k%args.model_save_interval == 0:\n",
    "        model.save_weights(model_file_name + '_epoch_{:02d}.hdf5'.format(k))\n",
    "\n",
    "model.save_weights(model_file_name + '_epoch_{:02d}.hdf5'.format(k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
